{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montgomery Police Traffic Stops Analysis with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This project explores the Montgomery Police traffic stops and it analyzes gender, race, time of the day and the rate at which subagencies carry out traffic stops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains traffic stopps by montgomery police officers. This dataset is gotten from https://data.montgomerycountymd.gov/Public-Safety/Traffic-Violations/4mse-ku6q and it is focused on the state of Maryland."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation/Cleaning\n",
    "\n",
    "A good analysis requires the data to be thoroughly examined and cleaned.\n",
    "A clean dataset makes the process easier to work with.\n",
    "Data preparation involves importing the dataset, handling missing  values, \n",
    "place holders, null values and fixing data type to the appropriate columns,\n",
    "droping less useful columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is from 01/01/2012 to 12/2/2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset into a DataFrame and naming it df\n",
    "df = pd.read_csv(r'C:\\Users\\Ice Asortse\\Desktop\\Traffic_Violations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming Time Of Stop column for beter manipulation\n",
    "df.rename(columns = {'Time Of Stop': 'Time','Violation Type': 'Violation'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining after renaming the Time column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the shape of the DataFrame\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset has 1,048,575 rows and 43 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examing the info the DataFrame\n",
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the missing values\n",
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to amount of missing value\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have a lot of missing values in some of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicates\n",
    "duplicates = df[df.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(duplicates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1,593 duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the duplicates\n",
    "df.drop_duplicates(keep='first',inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Checking the amount of duplicates\n",
    "duplicates = df[df.duplicated()]\n",
    "print(len(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for percentage of value counts for entries\n",
    "for col in df.columns:\n",
    "    print(col, '\\n', df[col].value_counts(normalize=True).head(10), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace place holders with nan\n",
    "df.replace(['?',], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make list of var containing missing values\n",
    "vars_with_na = [var for var in df.columns if df[var].isnull().sum()>1]\n",
    "\n",
    "#print var name and % of missing values\n",
    "for var in vars_with_na:\n",
    "    print(var, np.round(df[var].isnull().mean(),3), '% missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that are not useful for the project analysis\n",
    "drop_column = ['SeqID','Agency','Description','Location','Latitude','Longitude','Accident', \n",
    "               'Belts','Personal Injury', 'Property Damage','Fatal','Commercial License',\n",
    "               'HAZMAT','Commercial Vehicle','Alcohol','Work Zone','Search Conducted',\n",
    "               'Search Disposition','Search Outcome','Search Reason','Search Reason For Stop',\n",
    "               'Search Type','Search Arrest Reason','VehicleType','Model','Color','Charge','Article',\n",
    "               'Contributed To Accident','Arrest Type','Geolocation']\n",
    "df.drop(drop_column, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "data = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Examine data shape\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examing if there is any missing values left\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine if there is any null values left\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the shape of the clean dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clean dataset has 1,039,614 rows and 12 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting 'Year Of  Stop' to datatime \n",
    "data['Year Of Stop'] = pd.DatetimeIndex(data['Date Of Stop']).year\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the top 15 states with most stops\n",
    "pd.DataFrame(data['State'].value_counts()/len(data)).nlargest(15, columns = ['State'])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like cars from Maryland make up 87.4% of the stops in montgomery county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Stops by the months\n",
    "month = []\n",
    "for time_stamp in pd.to_datetime(data['Date Of Stop']):\n",
    "    month.append(time_stamp.month)\n",
    "m_count = pd.Series(month).value_counts()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(y=m_count.values, x=m_count.index, alpha=0.6)\n",
    "plt.title(\"Number of Stops Each Month\", fontsize=16)\n",
    "plt.xlabel(\"Month\", fontsize=16)\n",
    "plt.ylabel(\"No. of cars\", fontsize=16)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our visualization, March has the highest stops followed closely by May then April"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the year model of the vehicles stopped\n",
    "pd.DataFrame(data['Year'].value_counts()).nlargest(10, columns = ['Year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the 2006 cars are the most stopped followed closely by 2007 then 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the number of stops based on Race\n",
    "pd.DataFrame(data['Race'].value_counts()).nlargest(10, columns = ['Race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Examine the number of stops based on Gender\n",
    "pd.DataFrame(data['Gender'].value_counts()).nlargest(10, columns = ['Gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The male gender is most stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visuals for the Race and Gender disparity\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16,8))\n",
    "\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "sns.countplot(data['Gender'], ax=ax[0], color='blue')\n",
    "ax[0].set_title(\"Gender\", fontsize=14)\n",
    "\n",
    "sns.countplot(df['Race'], ax=ax[1], color='salmon')\n",
    "ax[1].set_title(\"Race\", fontsize=14)\n",
    "\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the city with the most stopped drivers\n",
    "violation_county = pd.DataFrame(data['Driver City'].\n",
    "value_counts()/len(data)).nlargest(10, columns = ['Driver City'])*100\n",
    "\n",
    "violation_county"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silver Spring has the most drivers stopped followed by Gaithersburg then Germantown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate 'Data Of Stop' and 'Time' (separated by a space)\n",
    "combined = data['Date Of Stop'].str.cat(data['Time'], sep = ' ')\n",
    "\n",
    "# Convert 'combined' to datetime format\n",
    "data['stop_datetime'] = pd.to_datetime(combined)\n",
    "\n",
    "# Examine the data type of 'stop_datetime'\n",
    "print(data.stop_datetime.dtype)\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index to 'stop_datetime'\n",
    "data.set_index('stop_datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print index to make sure \n",
    "print(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  index 'time_of_stops' by the hour\n",
    "time_of_stops = data.groupby(data.index.hour).Time.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line plot of 'hourly_arrest_rate'\n",
    "time_of_stops.plot(kind='bar', figsize=(16,8))\n",
    "\n",
    "# Add the xlabel, ylabel, and title\n",
    "plt.xlabel('Hour', fontsize=16)\n",
    "plt.ylabel('Number of Stops', fontsize=16)\n",
    "plt.title('Stops By the Hour', fontsize =20)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the unique values in 'violation'\n",
    "violations = pd.DataFrame(data.Violation.value_counts())\n",
    "\n",
    "\n",
    "# Express the counts as proportions\n",
    "violation_perct = pd.DataFrame(data.Violation.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the unique values in 'violations' and print\n",
    "violations = pd.DataFrame(data.Violation.value_counts())\n",
    "print(violations)\n",
    "\n",
    "print('----------------------')\n",
    "\n",
    "# Express the counts as proportions and print\n",
    "violation_perct = pd.DataFrame(data.Violation.value_counts(normalize = True))\n",
    "print(violation_perct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 'violations'\n",
    "violations.plot(kind='bar', color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As we can see moost of the stop end up with a citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of stops for each precint\n",
    "precints = pd.DataFrame(data.SubAgency.value_counts())\n",
    "precints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# express in percentage the number of stop for each precint\n",
    "precints_perct = pd.DataFrame(data.SubAgency.value_counts(normalize = True))*100\n",
    "precints_perct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 'precints'\n",
    "precints.plot(kind='bar', color='tan', figsize= (8,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Montgomery Police stops data analysis shows us alot about the department stops. From the data we are able to analyze alot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* More than 87% of the vehicle stops werevehicles from Maryland, followed by Virginia with 4.6% then DC with 2.4%.\n",
    "\n",
    "\n",
    "* More than 100,000 stops were made in the month of March and May. the least amount of stops made was in Decemeber.\n",
    "\n",
    "* About 370,000 stop made were white, folloed by black with about 320,000 then Hispanics with about 230,000.\n",
    "\n",
    "* More than 710,000 people stopped by the police were males and about 327,000 were females.\n",
    "\n",
    "* About 24% of the people stopped were from Silver Spring while 10% are from Gaithersburg, 8.4% were from Germantown and 7.8% from Rockville.\n",
    "\n",
    "* The most stopps happen between the hours of 10PM and 11PM followed by 8AM.\n",
    "\n",
    "* About 69% of the stops were citations while 29% were warnings.\n",
    "\n",
    "* about 24% of the stops were police from 4Th District, Wheaton, about 20% were from 3rd District, Silver Spring and 16% were from 2nd District, Bethesda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
